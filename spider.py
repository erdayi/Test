import os
import csv
import requests
from time import sleep
from random import uniform
from bs4 import BeautifulSoup
import warnings

warnings.filterwarnings("ignore")

# ==============================
# ğŸ› ï¸ ç”¨æˆ·é…ç½®åŒºï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
# ==============================
START_PAGE = 330           # â†â†â† ä»ç¬¬å‡ é¡µå¼€å§‹çˆ¬ï¼ˆä¾‹å¦‚ä¸Šæ¬¡æ–­åœ¨45é¡µï¼Œè¿™é‡Œè®¾46ï¼‰
END_PAGE_LIMIT = None        # å¯é€‰ï¼šæœ€å¤§çˆ¬åˆ°å¤šå°‘é¡µï¼ˆå¦‚ 100ï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
CSV_FILE = 'æ±½è½¦æŠ•è¯‰2025.csv'  # è¾“å‡ºæ–‡ä»¶å

# è¯·æ±‚å¤´
HEADERS = {
    "accept": "application/json, text/javascript, */*; q=0.01",
    "accept-encoding": "gzip, deflate, br, zstd",
    "accept-language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
    "connection": "keep-alive",
    # æ³¨æ„ï¼šcookie å¯èƒ½ä¼šè¿‡æœŸï¼å»ºè®®å®šæœŸæ›´æ–°æˆ–ä½¿ç”¨ Session è‡ªåŠ¨ç®¡ç†
    "cookie": "",
    "host": "www.aqsiqauto.com",  # âš ï¸ æ³¨æ„ï¼šrequests ä¼šè‡ªåŠ¨è®¾ç½® Hostï¼Œä¸€èˆ¬ä¸éœ€è¦æ‰‹åŠ¨å†™
    "sec-ch-ua": '"Chromium";v="142", "Microsoft Edge";v="142", "Not_A Brand";v="99"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-origin",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0",
    "x-requested-with": "XMLHttpRequest"
}

# å­—æ®µæ¸…ç†å‡½æ•°
def clean_field(text):
    return text.strip().replace('\n', ' ').replace('\r', ' ') if text else ''

# ä» HTML ä¸­æå–æ€»é¡µæ•°
def get_total_pages_from_html(html_text):
    soup = BeautifulSoup(html_text, "html.parser")
    last_link = soup.select_one("ul.yiiPager li.last a")
    if last_link and "page=" in last_link.get("href", ""):
        try:
            return int(last_link["href"].split("page=")[1].split("&")[0])
        except Exception:
            pass
    return None

# è·å–ä¸‹ä¸€é¡µ URL
def get_next_page_url(soup):
    next_elem = soup.select_one("ul.yiiPager li.next a")
    if next_elem and next_elem.get("href"):
        href = next_elem["href"]
        if href.startswith("/"):
            return "https://www.aqsiqauto.com" + href
    return None

# è§£æå½“å‰é¡µçš„æŠ•è¯‰è®°å½•
def parse_records(soup):
    tbody = soup.find("tbody", id="tb1")
    if not tbody:
        return []
    records = []
    for row in tbody.find_all("tr"):
        tds = row.find_all("td")
        if len(tds) < 7:
            continue

        comp_id = clean_field(tds[0].find("div").get_text() if tds[0].find("div") else tds[0].get_text())
        brand = clean_field(tds[1].get_text())
        series = clean_field(tds[2].get_text())
        model = clean_field(tds[3].get_text())
        summary = clean_field(tds[4].get_text())

        issue_td = tds[5]
        issue_divs = issue_td.find_all("div")
        if len(issue_divs) >= 2:
            main_issue = clean_field(issue_divs[0].get_text())
            sub_issue = clean_field(issue_divs[1].get_text())
        else:
            txt = clean_field(issue_td.get_text())
            if "â€”" in txt:
                parts = txt.split("â€”", 1)
                main_issue = parts[0].strip()
                sub_issue = parts[1].strip() if len(parts) > 1 else ""
            else:
                main_issue, sub_issue = txt, ""

        date = clean_field(tds[6].get_text())

        records.append({
            "æŠ•è¯‰ç¼–å·": comp_id,
            "æŠ•è¯‰å“ç‰Œ": brand,
            "æŠ•è¯‰è½¦ç³»": series,
            "æŠ•è¯‰è½¦å‹": model,
            "æŠ•è¯‰ç®€è¿°": summary,
            "æŠ•è¯‰é—®é¢˜": main_issue,
            "é—®é¢˜ç±»å‹": sub_issue,
            "æŠ•è¯‰æ—¥æœŸ": date
        })
    return records

# ==============================
# ğŸ” ä¸»çˆ¬è™«é€»è¾‘
# ==============================

# æ‰“å¼€ CSVï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
file_exists = os.path.isfile(CSV_FILE)
csv_file = open(CSV_FILE, mode='a', encoding='utf-8', newline='')
csv_writer = csv.DictWriter(csv_file, fieldnames=[
    'æŠ•è¯‰ç¼–å·', 'æŠ•è¯‰å“ç‰Œ', 'æŠ•è¯‰è½¦ç³»', 'æŠ•è¯‰è½¦å‹', 'æŠ•è¯‰ç®€è¿°', 'æŠ•è¯‰é—®é¢˜', 'é—®é¢˜ç±»å‹', 'æŠ•è¯‰æ—¥æœŸ'
])
if not file_exists:
    csv_writer.writeheader()
    print("ğŸ“ é¦–æ¬¡è¿è¡Œï¼Œå·²åˆ›å»º CSV æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ã€‚")

current_page = START_PAGE
total_pages = None
MAX_RETRIES = 3

try:
    while True:
        url = f"https://www.aqsiqauto.com/qichetousu.html?car_brand_id=0&car_series_id=0&page={current_page}&complaint_number=&complaint_status=3%2C4%2C5%2C7"
        print(f"æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ: {url}")

        retries = 0
        success = False

        while retries < MAX_RETRIES:
            try:
                response = requests.get(url, headers=HEADERS, timeout=15)
                response.encoding = 'utf-8'

                # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆªï¼ˆå†…å®¹è¿‡çŸ­ï¼‰
                if len(response.text) < 500:
                    raise Exception(f"å“åº”å†…å®¹è¿‡çŸ­ ({len(response.text)} å­—èŠ‚)ï¼Œç–‘ä¼¼åçˆ¬æ‹¦æˆª")

                soup = BeautifulSoup(response.text, "html.parser")

                # é¦–æ¬¡è·å–æ€»é¡µæ•°
                if total_pages is None:
                    total_pages = get_total_pages_from_html(response.text)
                    if total_pages:
                        print(f"ğŸ“Œ æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}")
                    else:
                        total_pages = 715  # é»˜è®¤å…œåº•
                        print("âš ï¸ æ— æ³•è·å–æ€»é¡µæ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 715")

                # è§£ææ•°æ® & ä¸‹ä¸€é¡µé“¾æ¥
                records = parse_records(soup)
                next_url = get_next_page_url(soup)

                print(f"âœ… ç¬¬ {current_page} é¡µä¿å­˜ {len(records)} æ¡è®°å½•")

                for rec in records:
                    csv_writer.writerow(rec)

                # å†³å®šæ˜¯å¦ç»§ç»­
                if next_url:
                    current_page += 1
                else:
                    print("ğŸ”š æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œçˆ¬å–ç»“æŸã€‚")
                    success = True
                    break

                success = True
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

            except Exception as e:
                retries += 1
                wait_sec = uniform(5, 10)
                print(f"âŒ ç¬¬ {current_page} é¡µå‡ºé”™ (å°è¯• {retries}/{MAX_RETRIES}): {e}")
                print(f"â³ ç­‰å¾… {wait_sec:.1f} ç§’åé‡è¯•...")
                sleep(wait_sec)

        if not success:
            print(f"ğŸ’¥ ç¬¬ {current_page} é¡µå¤šæ¬¡å¤±è´¥ï¼Œè·³è¿‡å¹¶å°è¯•ä¸‹ä¸€é¡µ")
            current_page += 1

        # å®‰å…¨å…œåº•ï¼šé˜²æ­¢æ— é™å¾ªç¯
        max_allowed = END_PAGE_LIMIT if END_PAGE_LIMIT else (total_pages or 1000)
        if current_page > max_allowed:
            print(f"ğŸ›‘ å·²è¾¾åˆ°è®¾å®šä¸Šé™ï¼ˆ{max_allowed} é¡µï¼‰ï¼Œå¼ºåˆ¶åœæ­¢ã€‚")
            break

        # æ­£å¸¸è¯·æ±‚é—´éš”
        sleep(uniform(2, 4))

except KeyboardInterrupt:
    print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–ã€‚")
finally:
    csv_file.close()
    print("ğŸ’¾ CSV æ–‡ä»¶å·²å…³é—­ã€‚")
    print("ğŸ‰ çˆ¬å–æµç¨‹ç»“æŸã€‚")